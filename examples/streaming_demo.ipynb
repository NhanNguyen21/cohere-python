{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33eb259f-54b8-469a-b97c-d5b90612c53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")  # make sure we can run this from the repo\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import cohere\n",
    "CHAT_MODEL = 'converse-xlarge-nightly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0b6ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0.53s] StreamingText(id='b65ddd16-d4d6-4812-8d4c-6d3cb7d18d17', session_id='chat-d489c39a-e152-49da-9ddc-9801bd74d823-26d52712-c07c-4a95-b708-c465a86c808d', index=0, text='I')\n",
      "[1][0.53s] StreamingText(id=None, session_id=None, index=0, text=\"'m\")\n",
      "[2][0.54s] StreamingText(id=None, session_id=None, index=0, text=' doing')\n",
      "[3][0.57s] StreamingText(id=None, session_id=None, index=0, text=' well')\n",
      "[4][0.60s] StreamingText(id=None, session_id=None, index=0, text=',')\n",
      "[5][0.63s] StreamingText(id=None, session_id=None, index=0, text=' thank')\n",
      "[6][0.67s] StreamingText(id=None, session_id=None, index=0, text=' you')\n",
      "[7][0.68s] StreamingText(id=None, session_id=None, index=0, text='!')\n",
      "[8][0.71s] StreamingText(id=None, session_id=None, index=0, text=' How')\n",
      "[9][0.74s] StreamingText(id=None, session_id=None, index=0, text=' about')\n",
      "[10][0.77s] StreamingText(id=None, session_id=None, index=0, text=' you')\n",
      "[11][0.80s] StreamingText(id=None, session_id=None, index=0, text='?')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['b65ddd16-d4d6-4812-8d4c-6d3cb7d18d17'],\n",
       " [\"I'm doing well, thank you! How about you?\"])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = cohere.Client()\n",
    "start_time = time.time()\n",
    "streaming_gens = co.chat(query=\"Hey! Don't worry, üêù happy~\", max_tokens=20, stream=True)\n",
    "for i, token in enumerate(streaming_gens):\n",
    "    print(f\"[{i}][{time.time()-start_time:.2f}s] {token}\")\n",
    "# the request id is available after the first token has streamed, and response so far is in texts\n",
    "streaming_gens.ids, streaming_gens.texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7348c95-7b3b-4ad4-b8cd-c600c8f2943a",
   "metadata": {},
   "source": [
    "## Client example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f90b4",
   "metadata": {},
   "source": [
    "### Simple Generate Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed8a61-f0de-489a-ad5e-9055fbd40781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0.64s] StreamingText(id='81ab6932-87ee-40dc-8e02-de9a2738c811', index=0, text=' üêù')\n",
      "[1][0.67s] StreamingText(id=None, index=0, text='\\n')\n",
      "[2][0.70s] StreamingText(id=None, index=0, text='\\n')\n",
      "[3][0.73s] StreamingText(id=None, index=0, text='1')\n",
      "[4][0.76s] StreamingText(id=None, index=0, text=')')\n",
      "[5][0.79s] StreamingText(id=None, index=0, text=' I')\n",
      "[6][0.83s] StreamingText(id=None, index=0, text=' think')\n",
      "[7][0.84s] StreamingText(id=None, index=0, text=' this')\n",
      "[8][0.87s] StreamingText(id=None, index=0, text=' one')\n",
      "[9][0.90s] StreamingText(id=None, index=0, text=' is')\n",
      "[10][0.93s] StreamingText(id=None, index=0, text=' the')\n",
      "[11][0.95s] StreamingText(id=None, index=0, text=' main')\n",
      "[12][0.98s] StreamingText(id=None, index=0, text=' reason')\n",
      "[13][1.01s] StreamingText(id=None, index=0, text='.')\n",
      "[14][1.04s] StreamingText(id=None, index=0, text=' I')\n",
      "[15][1.06s] StreamingText(id=None, index=0, text=\"'m\")\n",
      "[16][1.09s] StreamingText(id=None, index=0, text=' afraid')\n",
      "[17][1.12s] StreamingText(id=None, index=0, text=' that')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['81ab6932-87ee-40dc-8e02-de9a2738c811'],\n",
       " [\" üêù\\n\\n1) I think this one is the main reason. I'm afraid that\"])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = cohere.Client()\n",
    "start_time = time.time()\n",
    "streaming_gens = co.generate(prompt=\"Hey! Don't worry, üêù happy~\", max_tokens=20, stream=True)\n",
    "for i, token in enumerate(streaming_gens):\n",
    "    print(f\"[{i}][{time.time()-start_time:.2f}s] {token}\")\n",
    "# the request id is available after the first token has streamed, and response so far is in texts\n",
    "streaming_gens.ids, streaming_gens.texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8140b9",
   "metadata": {},
   "source": [
    "### Widgets Chat Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a484c0e-9f6b-48ef-9e3f-e6795f52b425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "co = cohere.Client()\n",
    "max_tokens = 250\n",
    "\n",
    "text = \"\"\n",
    "output_area = display(\"Type your message below and hit enter to chat!\", display_id=True)\n",
    "input_text = widgets.Text(value=\"Hey!\", description=\"User:\", disabled=False)\n",
    "\n",
    "def append_text(s):\n",
    "    global text\n",
    "    text += s\n",
    "    output_area.update(Markdown(text))\n",
    "\n",
    "def stream_response(input_text):\n",
    "    query = input_text.value\n",
    "    if query.startswith(\"<generating\"):\n",
    "        return\n",
    "    input_text.value = \"<generating response>\"\n",
    "    append_text(f\"\\n\\nUser: {query}\\n\\nChatbot: \")\n",
    "    streaming_chat = co.chat(\n",
    "        query=query,\n",
    "        max_tokens=max_tokens,\n",
    "        model=CHAT_MODEL,\n",
    "        stream=True,\n",
    "    )\n",
    "    for token in streaming_chat:\n",
    "        append_text(token.text)\n",
    "    input_text.value = \"\"\n",
    "\n",
    "display(input_text)\n",
    "input_text.on_submit(stream_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f2602-f3ae-4449-8f61-2be6352a04ad",
   "metadata": {},
   "source": [
    "## AsyncClient example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049f74f-e9ae-4fc3-a695-c1b585f0916c",
   "metadata": {},
   "source": [
    "### Widgets generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219446b-6788-48d8-8de4-af8f85e55cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ÏïàÎÖïÌïò\"\n",
    "num_generations = 5\n",
    "max_tokens = 50\n",
    "texts = [f\"**Generation #{i+1}:** {prompt}\" for i in range(num_generations)]\n",
    "displays = [display(display_id=True) for t in texts]\n",
    "\n",
    "async with cohere.AsyncClient() as aio_co:\n",
    "    start_time = time.time()\n",
    "    aio_streaming_gens = await aio_co.generate(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        model=\"command-xlarge-nightly\",\n",
    "        num_generations=num_generations,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for token in aio_streaming_gens:\n",
    "        texts[token.index] += token.text\n",
    "        displays[token.index].update(Markdown(texts[token.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cea95-c0ce-414a-9aa7-162bb8179e11",
   "metadata": {},
   "source": [
    "### Simple Chat Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3c5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Do You Want to Build a Snowman?\"\n",
    "async with cohere.AsyncClient() as aio_co:\n",
    "    streaming_chat = await aio_co.chat(\n",
    "        query=query,\n",
    "        max_tokens=50,\n",
    "        model=CHAT_MODEL,\n",
    "        stream=True,\n",
    "    )\n",
    "    async for token in streaming_chat:\n",
    "        print(token)\n",
    "    print(streaming_chat.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87ff73",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 2 required positional arguments: 'id' and 'session_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m streaming_gens \u001b[39m=\u001b[39m co\u001b[39m.\u001b[39mchat(query\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHey! Don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt worry, üêù happy~\u001b[39m\u001b[39m\"\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(streaming_gens):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m][\u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms] \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# the request id is available after the first token has streamed, and response so far is in texts\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Cohere/cohere-python/examples/../cohere/responses/chat.py:112\u001b[0m, in \u001b[0;36mStreamingChat.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFor AsyncClient, use `async for` to iterate through the `StreamingChat`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39miter_lines():\n\u001b[0;32m--> 112\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_response_item(line)\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[39myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/Documents/Cohere/cohere-python/examples/../cohere/responses/chat.py:105\u001b[0m, in \u001b[0;36mStreamingChat._make_response_item\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtexts[index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m text\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m StreamingText(index\u001b[39m=\u001b[39;49mindex, text\u001b[39m=\u001b[39;49mtext)\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 2 required positional arguments: 'id' and 'session_id'"
     ]
    }
   ],
   "source": [
    "co = cohere.Client()\n",
    "start_time = time.time()\n",
    "streaming_gens = co.chat(query=\"Hey! Don't worry, üêù happy~\", max_tokens=20, stream=True)\n",
    "for i, token in enumerate(streaming_gens):\n",
    "    print(f\"[{i}][{time.time()-start_time:.2f}s] {token}\")\n",
    "# the request id is available after the first token has streamed, and response so far is in texts\n",
    "streaming_gens.ids, streaming_gens.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2fb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
